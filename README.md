# <p align=center>`PRIME for Video Anomaly Detection`</p><!-- omit in toc -->

Kaige Li, Weiming Shi, and Xiaochun Cao*, IEEE Senior Member

*Corresponding author: [Xiaochun Cao](https://scholar.google.com/citations?user=PDgp6OkAAAAJ&hl=en).

## Table of Contents

  * [Introduction](#1-introduction)
  * [Environment Setup](#2-Environment-Setup)
  * [Dataset](#3-Dataset-Setup)
  * [Framework Structure](#4-Framework-Structure)
  * [Acknowledgements](#5-Acknowledgements)
  * [Future Work](#6-future-work)


## Code Implementation Statement

As discussed in [8](https://github.com/lhoyer/MIC/issues/8), [54](https://github.com/lhoyer/MIC/issues/54) and [63](https://github.com/lhoyer/MIC/issues/63), **our method inherits the instability of [MIC](https://github.com/lhoyer/MIC).** :cry:

Note, however, that the mathematical expectation of performance is the same for both, i.e., **76.9%** mIoU and **69.9%** mIoU on GTAVâ†’Cityscapes and SYNTHIAâ†’Cityscapes, respectively. :100:

## 1. Introduction

 ðŸ”¥ Pending


## 2. Environment Setup

First, please install cuda version 11.0.3 available at [https://developer.nvidia.com/cuda-11-0-3-download-archive](https://developer.nvidia.com/cuda-11-0-3-download-archive). It is required to build mmcv-full later.

For this project, we used python 3.8.5. We recommend setting up a new virtual
environment:

```shell
python -m venv ~/venv/LVP-UDASeg
source ~/venv/LVP-UDASeg/bin/activate
```

In that environment, the requirements can be installed with:

```shell
pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html
pip install mmcv-full==1.3.7  # requires the other packages to be installed first
```

Further, please download the MiT weights from SegFormer using the
following script. If problems occur with the automatic download, please follow
the instructions for a manual download within the script.

```shell
sh tools/download_checkpoints.sh
```

## 3. Dataset Setup

**Cityscapes:** Please, download leftImg8bit_trainvaltest.zip and
gt_trainvaltest.zip from [here](https://www.cityscapes-dataset.com/downloads/)
and extract them to `data/cityscapes`.

**GTA:** Please, download all image and label packages from
[here](https://download.visinf.tu-darmstadt.de/data/from_games/) and extract
them to `data/gta`.


The final folder structure should look like this:

```none
LVP
â”œâ”€â”€ ...
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ cityscapes
â”‚   â”‚   â”œâ”€â”€ leftImg8bit
â”‚   â”‚   â”‚   â”œâ”€â”€ train
â”‚   â”‚   â”‚   â”œâ”€â”€ val
â”‚   â”‚   â”œâ”€â”€ gtFine
â”‚   â”‚   â”‚   â”œâ”€â”€ train
â”‚   â”‚   â”‚   â”œâ”€â”€ val
â”‚   â”œâ”€â”€ gta
â”‚   â”‚   â”œâ”€â”€ images
â”‚   â”‚   â”œâ”€â”€ labels
â”œâ”€â”€ ...
```

**Data Preprocessing:** Finally, please run the following scripts to convert the label IDs to the
train IDs and to generate the class index for RCS:

```shell
python tools/convert_datasets/gta.py data/gta --nproc 8
python tools/convert_datasets/cityscapes.py data/cityscapes --nproc 8
python tools/convert_datasets/synthia.py data/synthia/ --nproc 8
```

## 4. Framework Structure

This project is based on [mmsegmentation version 0.16.0](https://github.com/open-mmlab/mmsegmentation/tree/v0.16.0).
For more information about the framework structure and the config system,
please refer to the [mmsegmentation documentation](https://mmsegmentation.readthedocs.io/en/latest/index.html)
and the [mmcv documentation](https://mmcv.readthedocs.ihttps://arxiv.org/abs/2007.08702o/en/v1.3.7/index.html).


ðŸ”‘ **Key Idea**

Our Language-Vision Prior (LVP) combines:

* Language Prior (LP): multi-prototype prompts capture class-level semantics and intra-class variance.

* Vision Prior (VP): bi-directional masking encourages robust global-local reasoning.

Together, they guide stable and reliable domain adaptation.

**Overall Training Pseudocode**

```python
import numpy as np

def SELECTCANDIDATE(P, SP):
    """
    è®ºæ–‡1-86èŠ‚Pareto-based candidate selectionçš„äºŒå€¼æŒ‡æ ‡é€‚é…å®žçŽ°
    Input:
        P: å€™é€‰æ± ï¼Œåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå€™é€‰æç¤ºï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼Œæè¿°æç¤ºæ ¸å¿ƒé€»è¾‘ï¼‰
        SP: äºŒå€¼ç»“æžœçŸ©é˜µï¼Œnumpyæ•°ç»„ï¼Œshape=(len(P), len(å®žä¾‹é›†))ï¼ŒSP[k][i] âˆˆ {0,1}ï¼ˆ0=é”™è¯¯ï¼Œ1=æ­£ç¡®ï¼‰
    Output:
        selected_idx: æŠ½æ ·é€‰ä¸­çš„å€™é€‰åœ¨Pä¸­çš„ç´¢å¼•
    """
    # 1. æž„å»ºæ¯ä¸ªå®žä¾‹içš„æœ€ä¼˜å€™é€‰é›†åˆP*[i]ï¼ˆè®ºæ–‡1-86èŠ‚æ­¥éª¤2-5ï¼‰
    num_instances = SP.shape[1]  # å®žä¾‹æ€»æ•°
    P_star = [[] for _ in range(num_instances)]  # P_star[i]ä¸ºå®žä¾‹içš„æœ€ä¼˜å€™é€‰é›†åˆ
    for i in range(num_instances):
        # å¯¹å®žä¾‹iï¼Œæ‰¾åˆ°æ‰€æœ‰é¢„æµ‹æ­£ç¡®ï¼ˆSP[k][i]==1ï¼‰çš„å€™é€‰ç´¢å¼•k
        best_candidates_idx = np.where(SP[:, i] == 1)[0].tolist()
        # è‹¥æ‰€æœ‰å€™é€‰å‡é”™è¯¯ï¼Œä¿ç•™å…¨éƒ¨å€™é€‰ï¼ˆé¿å…ç­›é€‰ä¸­æ–­ï¼Œè®ºæ–‡1-73èŠ‚å€™é€‰æ± ä¿ç•™é€»è¾‘ï¼‰
        if not best_candidates_idx:
            best_candidates_idx = list(range(len(P)))
        P_star[i] = [P[k] for k in best_candidates_idx]  # å­˜å‚¨å€™é€‰å¯¹è±¡
    
    # 2. æ•´åˆå…¨å±€å€™é€‰æ± Cï¼ˆè®ºæ–‡1-86èŠ‚æ­¥éª¤6-7ï¼šåŽ»é‡ï¼‰
    C = []
    for candidates in P_star:
        for cand in candidates:
            if cand not in C:
                C.append(cand)
    # å‰”é™¤â€œåœ¨æ‰€æœ‰å®žä¾‹ä¸Šå‡é”™è¯¯â€çš„å€™é€‰ï¼ˆè®ºæ–‡1-86èŠ‚éšå«é€»è¾‘ï¼šä¿ç•™æœ‰æ•ˆç­–ç•¥ï¼‰
    valid_candidates = []
    for cand in C:
        cand_idx = P.index(cand)
        if np.any(SP[cand_idx] == 1):
            valid_candidates.append(cand)
    C = valid_candidates
    if not C:
        return 0  # æžç«¯æƒ…å†µï¼šæ— æœ‰æ•ˆå€™é€‰ï¼Œè¿”å›žåˆå§‹å€™é€‰
    
    # 3. å‰”é™¤ä¸¥æ ¼æ”¯é…å€™é€‰ï¼ˆè®ºæ–‡1-86èŠ‚æ­¥éª¤8-13ï¼‰
    D = []  # å­˜å‚¨è¢«æ”¯é…çš„å€™é€‰
    for idx_x in range(len(C)):
        x = C[idx_x]
        x_idx = P.index(x)  # xåœ¨åŽŸå§‹å€™é€‰æ± Pä¸­çš„ç´¢å¼•
        x_correct = set(np.where(SP[x_idx] == 1)[0].tolist())  # xçš„æ­£ç¡®å®žä¾‹é›†åˆ
        # æ£€æŸ¥xæ˜¯å¦è¢«å…¶ä»–å€™é€‰æ”¯é…
        is_dominated = False
        for idx_y in range(len(C)):
            if idx_x == idx_y:
                continue
            y = C[idx_y]
            y_idx = P.index(y)
            y_correct = set(np.where(SP[y_idx] == 1)[0].tolist())  # yçš„æ­£ç¡®å®žä¾‹é›†åˆ
            # æ”¯é…æ¡ä»¶ï¼šxçš„æ­£ç¡®é›†åˆè¢«yåŒ…å«ï¼Œä¸”yæœ‰xæœªè¦†ç›–çš„æ­£ç¡®å®žä¾‹ï¼ˆè®ºæ–‡1-86èŠ‚æ”¯é…å®šä¹‰é€‚é…ï¼‰
            if x_correct.issubset(y_correct) and len(y_correct - x_correct) > 0:
                is_dominated = True
                break
        if is_dominated:
            D.append(x)
    # ä¿®å‰ªå€™é€‰æ± ï¼šç§»é™¤è¢«æ”¯é…å€™é€‰ï¼Œå¾—åˆ°Ë†Cï¼ˆè®ºæ–‡1-86èŠ‚æ­¥éª¤13ï¼‰
    C_hat = [cand for cand in C if cand not in D]
    if len(C_hat) == 1:
        return P.index(C_hat[0])  # ä»…1ä¸ªå€™é€‰ï¼Œç›´æŽ¥è¿”å›ž
    
    # 4. æŒ‰f[Î¦k]æ¦‚çŽ‡æŠ½æ ·ï¼ˆè®ºæ–‡1-86èŠ‚æ­¥éª¤14-16ï¼šf[Î¦k]ä¸ºå€™é€‰è¿›å…¥P*[i]çš„å®žä¾‹æ•°ï¼‰
    f = []
    for cand in C_hat:
        cand_idx = P.index(cand)
        # f[Î¦k] = å€™é€‰åœ¨æ‰€æœ‰å®žä¾‹ä¸Šæ­£ç¡®çš„æ•°é‡ï¼ˆå³è¿›å…¥P*[i]çš„å®žä¾‹æ•°ï¼‰
        f_k = np.sum(SP[cand_idx] == 1)
        f.append(f_k)
    # æŒ‰f[k]æ­£æ¯”æŠ½æ ·ï¼ˆå¦‚æƒé‡æŠ½æ ·ï¼‰
    total_f = sum(f)
    probabilities = [fk / total_f for fk in f]
    selected_cand = np.random.choice(C_hat, p=probabilities)
    return P.index(selected_cand)

# ------------------------------ ç¤ºä¾‹è¾“å…¥ ------------------------------
# 1. å€™é€‰æ± Pï¼š3ä¸ªè§†é¢‘å¼‚å¸¸æ£€æµ‹çš„æç¤ºå€™é€‰ï¼ˆæè¿°æ ¸å¿ƒé€»è¾‘ï¼Œå¯¹åº”å‰æ–‡A/B/Cï¼‰
P = [
    # å€™é€‰Aï¼šä»…å…³æ³¨è¡Œä¸ºå¼‚å¸¸çš„è‚¢ä½“å§¿æ€å˜åŒ–
    "æå–å¼‚å¸¸ç‰¹å¾æ—¶ï¼Œä»…èšç„¦è¿žç»­å¸§ä¸­è¡Œäººçš„è‚¢ä½“å§¿æ€å˜åŒ–ï¼Œåˆ¤å®šè¡Œä¸ºå¼‚å¸¸",
    # å€™é€‰Bï¼šå…³æ³¨è¡Œä¸º+ç‰©ä½“å¼‚å¸¸çš„å…³é”®ç‰¹å¾
    "æå–å¼‚å¸¸ç‰¹å¾æ—¶ï¼Œè¦†ç›–è¡Œäººè‚¢ä½“å§¿æ€å˜åŒ–ã€ç‰©ä½“ç»“æž„å®Œæ•´æ€§ï¼Œåˆ¤å®šè¡Œä¸º/ç‰©ä½“å¼‚å¸¸",
    # å€™é€‰Cï¼šè¦†ç›–è¡Œä¸º+ç‰©ä½“+çŽ¯å¢ƒå¼‚å¸¸çš„å…¨é¢ç‰¹å¾
    "æå–å¼‚å¸¸ç‰¹å¾æ—¶ï¼ŒåŒ…å«è‚¢ä½“å§¿æ€ã€ç‰©ä½“ç»“æž„ã€çŽ¯å¢ƒå…‰å¼º/çº¹ç†å˜åŒ–ï¼Œåˆ¤å®šå…¨ç±»åˆ«å¼‚å¸¸"
]

# 2. äºŒå€¼ç»“æžœçŸ©é˜µSPï¼šshape=(3ä¸ªå€™é€‰, 5ä¸ªå®žä¾‹)ï¼ŒSP[k][i]è¡¨ç¤ºå€™é€‰kåœ¨å®žä¾‹iä¸Šçš„é¢„æµ‹ç»“æžœï¼ˆ1=æ­£ç¡®ï¼Œ0=é”™è¯¯ï¼‰
# å®žä¾‹é¡ºåºï¼ši=0(è¡Œä¸º-è·Œå€’)ã€i=1(è¡Œä¸º-å¥”è·‘)ã€i=2(ç‰©ä½“-è´§æž¶åå¡Œ)ã€i=3(çŽ¯å¢ƒ-ç¯å…‰éª¤æš—)ã€i=4(çŽ¯å¢ƒ-åœ°é¢ç§¯æ°´)
SP = np.array([
    [1, 0, 0, 0, 0],  # å€™é€‰Aï¼šä»…æ­£ç¡®å®žä¾‹0
    [1, 1, 1, 0, 0],  # å€™é€‰Bï¼šæ­£ç¡®å®žä¾‹0ã€1ã€2
    [1, 1, 1, 1, 1]   # å€™é€‰Cï¼šæ­£ç¡®æ‰€æœ‰å®žä¾‹
])

# ------------------------------ å‡½æ•°è°ƒç”¨ä¸Žè¾“å‡º ------------------------------
np.random.seed(42)  # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æžœå¯å¤çŽ°
selected_idx = SELECTCANDIDATE(P, SP)
print(f"é€‰ä¸­çš„å€™é€‰ç´¢å¼•ï¼š{selected_idx}")
print(f"é€‰ä¸­çš„å€™é€‰æç¤ºï¼š{P[selected_idx]}")
```

##  5. Acknowledgements

TIP is based on the following open-source projects. We thank their
authors for making the source code publicly available.

* [DAFormer](https://github.com/lhoyer/DAFormer)
* [MMSegmentation](https://github.com/open-mmlab/mmsegmentation)
* [SegFormer](https://github.com/NVlabs/SegFormer)
* [DACS](https://github.com/vikolss/DACS)

##  6. Future Work


## Multi-Prototype Representation

Current results indicate that **small-object classes** (e.g., *traffic light*, *traffic sign*, *pole*) show higher intra-class diversity, while **large-area classes** (e.g., *road*, *sky*) appear more homogeneous. Using a single prototype per class may not be sufficient to capture such diversity.

### Directions

- **Adaptive Prototype Allocation**
  - Allocate prototypes per class based on:
    - *Intra-class diversity* (e.g., covariance trace, mean pairwise distance).
    - *Effective sample size* (e.g., log of pixel count).
    - *Resource budget* (global prototype limit with min/max constraints).

- **Dynamic Selection**
  - Explore automatic methods to determine prototype counts:
    - *k-means* with silhouette or Daviesâ€“Bouldin scores.
    - *Gaussian Mixture Models* with BIC/AIC.

- **Class-Specific Strategies**
  - Small-object classes with heterogeneous appearance â†’ more prototypes.
  - Large-object classes with stable texture â†’ fewer prototypes.

- **Evaluation Metrics**
  - Monitor **intra-class coverage** (distance to nearest prototype).
  - Monitor **inter-class separation** (margin to non-class prototypes).
  - Use these signals to refine prototype allocation.

---

*The goal is to better capture intra-class variability without overspending resources, paving the way for finer-grained representation and improved segmentation quality.*



## Prototype Allocation

This repository provides a utility function to allocate prototype counts per class  
based on intra-class diversity and sample size.

## Example: Allocate Prototypes

```python

import torch
import math

def allocate_prototypes(feats_by_class, K_total, K_min=1, K_max=10, alpha=0.7, beta=0.3, eps=1e-8):
    """
    Allocate prototype counts per class based on intra-class diversity and sample size.

    Args:
        feats_by_class (dict[int, torch.Tensor]): A dictionary mapping class -> features (N_c, C).
        K_total (int): Total number of prototypes across all classes.
        K_min (int): Minimum number of prototypes per class (default=1).
        K_max (int): Maximum number of prototypes per class (default=10).
        alpha (float): Weight for diversity in allocation (default=0.7).
        beta (float): Weight for sample count in allocation (default=0.3).
        eps (float): Small epsilon to avoid division by zero.

    Returns:
        dict[int, int]: A dictionary mapping each class to its allocated number of prototypes.
    """
    classes = sorted(feats_by_class.keys())
    D, L = [], []  # store diversity and log-count values

    for c in classes:
        X = feats_by_class[c]
        # Use covariance trace as a measure of diversity
        Xc = X - X.mean(dim=0, keepdim=True)
        cov_trace = (Xc.T @ Xc / max(1, X.shape[0]-1)).diag().sum().item()
        D.append(max(cov_trace, 0.0))
        L.append(math.log1p(X.shape[0]))  # log(1 + sample size)

    # Normalize diversity and sample size contributions
    D_sum = sum(D) + eps
    L_sum = sum(L) + eps
    d_hat = [d / D_sum for d in D]
    n_hat = [l / L_sum for l in L]

    # Initial allocation: ensure each class has at least K_min
    base = K_min * len(classes)
    room = max(K_total - base, 0)
    q = [alpha * d + beta * n for d, n in zip(d_hat, n_hat)]
    q_sum = sum(q) + eps
    k_float = [K_min + room * (qi / q_sum) for qi in q]  # float allocation

    # Round allocations and apply min/max limits
    k_round = [int(round(x)) for x in k_float]
    k_round = [max(K_min, min(K_max, k)) for k in k_round]

    # Adjust to make sure the total sum equals K_total
    diff = K_total - sum(k_round)
    if diff != 0:
        # Priority: adjust classes whose rounded value deviates most from float target
        prio = sorted(
            range(len(classes)),
            key=lambda i: (k_float[i] - k_round[i]),
            reverse=(diff > 0),
        )
        i = 0
        while diff != 0 and i < len(prio):
            idx = prio[i]
            newk = k_round[idx] + (1 if diff > 0 else -1)
            if K_min <= newk <= K_max:
                k_round[idx] = newk
                diff += -1 if diff > 0 else 1
            i += 1

    K_dict = {c: k for c, k in zip(classes, k_round)}
    return K_dict

```




## Code Availability Statement
This code is associated with a paper currently under review. To comply with the review process, the code will be made FULLY available once the paper is accepted.  :smiley:

We appreciate your understanding and patience. Once the code is released, we will warmly welcome any feedback and suggestions. Please stay tuned for our updates!
